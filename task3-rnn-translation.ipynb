{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJCMCx6ISZZw"
   },
   "source": [
    "## **Task 3: RNN Application -- Neural Machine Translation** (25%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8wjQhNtSffW"
   },
   "source": [
    "In this task, you are going to perform neural machine translation (NMT). NMT involves using a neural network to translate from one language to another. This is a widely studied natural language processing (NLP) problem and has tremendous real-world applications.\n",
    "\n",
    "Machine Translation is a challenging task that involves both the usage of complex architectures and data processing tricks to obtain human-level performance. **In this notebook, you will implement a simple Seq2Seq architecture using RNN layers in keras.**\n",
    "\n",
    "**The goal is to train a model to translate from Dutch (input language) to English (target language)**. This notebook uses data from the [Tab Delimited Bilingual Sentence Pairs](https://www.manythings.org/anki/) repository. You can find many such language pairs here.\n",
    "\n",
    "## <span style=\"color:red\"><strong>NOTE: Training this model may take 10-15 minutes of time depending on the strength of the system, so please plan accordingly.</strong></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UZ_G4XdfP7GK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MjTYqMoN8fh"
   },
   "source": [
    "### Broad Overview of Steps:\n",
    "1. Preprocess and encode data\n",
    "2. Create dataset/dataloaders\n",
    "3. Define Model Architecture\n",
    "4. Train Model\n",
    "5. Evaluate results\n",
    "\n",
    "Step 1. has already been completed for you. We provide two .npy files that contain the data: \n",
    "- `nmt_eng.npy` contains the encoded English sentences.\n",
    "- `nmt_nl.npy` contains the encoded Dutch sentences. \n",
    "The sentences already have been normalized, padded and appended with the \\<start\\> and \\<end\\> tokens.\n",
    "\n",
    "We also provide two vocabulary files `eng_vocab.txt` and `nl_vocab.txt` for the English and Dutch languages respectively. The vocabulary files will be used for decoding the input and output of our model.\n",
    "\n",
    "## Part 1. Load Encoded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO:</strong></font> Execute the following cells to load the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F55EwI6RQl1A",
    "outputId": "1e087591-3dad-4471-97df-d9af9214dddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of english vocab: 9044\n",
      "Size of dutch vocab: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load Vocabulary files (dictionaries of word:int pairs)\n",
    "with open(\"text_data/eng_vocab.txt\", 'r') as f:\n",
    "    eng_vocab = json.load(f)\n",
    "\n",
    "with open(\"text_data/nl_vocab.txt\", 'r') as f:\n",
    "    nl_vocab = json.load(f)\n",
    "    \n",
    "eng_vocab = {int(key): value for key, value in eng_vocab.items()}\n",
    "nl_vocab = {int(key): value for key, value in nl_vocab.items()}\n",
    "\n",
    "print(f'Size of english vocab: {len(eng_vocab)}')\n",
    "print(f'Size of dutch vocab: {len(nl_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of english text data: (75298, 30)\n",
      "Shape of dutch text data: (75298, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load Encoded Sentence Data\n",
    "eng_text = np.load(\"text_data/nmt_eng.npy\")\n",
    "nl_text = np.load(\"text_data/nmt_nl.npy\")\n",
    "\n",
    "print(f'Shape of english text data: {eng_text.shape}')\n",
    "print(f'Shape of dutch text data: {nl_text.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmjy9sPDOCnY"
   },
   "source": [
    "## Part 2: Datasets and Dataloading (3%)\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> <b>Complete the functions in utils/translation/text_data.py</b>\n",
    "\n",
    "This will create the train, validation, and test datasets for our translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-Mx6WgMBVI3T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 67769\n",
      "Validation size: 7529\n"
     ]
    }
   ],
   "source": [
    "from utils.translation.text_data import get_dataset, get_dataset_partitions_tf, decode_text\n",
    "\n",
    "text_ds = get_dataset(nl_text, eng_text)\n",
    "train_ds, val_ds = get_dataset_partitions_tf(text_ds, len(text_ds))\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Validation size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       " array([  2,  18,  71,  17,   9,  36, 640,   7,   3,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0])>,\n",
       " <tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       " array([ 2, 62, 47,  6, 67, 13, 77,  8,  3,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at a sample from the dataset\n",
    "sample = next(iter(train_ds))\n",
    "sample[0], sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NL: ['[SOS]', 'van', 'wie', 'heb', 'je', 'dit', 'gekregen', '?', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "EN: ['[SOS]', 'who', 'did', 'you', 'get', 'that', 'from', '?', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "decoded_nl = decode_text(sample[0].numpy(), vocab=nl_vocab)\n",
    "decoded_eng = decode_text(sample[1].numpy(), vocab=eng_vocab)\n",
    "print('NL:', decoded_nl)\n",
    "print()\n",
    "print('EN:', decoded_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBiLRGc7RL-g"
   },
   "source": [
    "## Part 3: Model Architecture (15%)\n",
    "\n",
    "### Seq2Seq Model\n",
    "\n",
    "In the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning \"sequence-to-sequence models\" with no further context. Here's how it works (This example is English to French):\n",
    "\n",
    "- An RNN layer (or stack thereof) acts as \"encoder\": it processes the input sequence and returns its own internal state. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n",
    "- Another RNN layer (or stack thereof) acts as \"decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence.\n",
    "\n",
    "![teacher_forcing](./img/seq2seq-teacher-forcing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In inference mode, i.e. when we want to decode unknown input sequences, we go through a slightly different process:\n",
    "\n",
    "1) Encode the input sequence into state vectors.\n",
    "2) Start with a target sequence of size 1 (just the start-of-sequence character).\n",
    "3) Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character.\n",
    "4) Sample the next character using these predictions (we simply use argmax).\n",
    "5) Append the sampled character to the target sequence\n",
    "6) Repeat until we generate the end-of-sequence character or we hit the character limit.\n",
    "\n",
    "![seq2seq-inference](./img/seq2seq-inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq model implementation requires a more complex setup than what is provided by keras.Sequential().\n",
    "You will be exposed to writing modular code using custom `keras.layer` and `keras.Model` classes. **First, please read https://keras.io/guides/making_new_layers_and_models_via_subclassing/** to get an idea about writing custom modules in tensorflow/keras, which is what is done in practice to implement complex architectures.\n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> <b>Based on the above, you need to complete the code in utils/translation/layers.py</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH, PREFETCH, CACHE the datasets\n",
    "#You can change the batch size based on memory requirements\n",
    "BATCH_SIZE = 64\n",
    "train_loader = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()\n",
    "val_loader = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.translation.layers import TranslationModel\n",
    "\n",
    "eng_vocab_size = len(eng_vocab)\n",
    "nl_vocab_size = len(nl_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "#Initialize Model\n",
    "model = TranslationModel(nl_vocab_size, eng_vocab_size, hidden_size, eng_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training the Model (5%)\n",
    "\n",
    "The following cell(s) will train your Machine Translation model. The loss function used is Cross Entropy (since we are performing classification across the vocabulary at each time step). In practice, we usually implement a machine translation metric such as BLEU or ROUGE ([reference](https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb#:~:text=While%20BLEU%20score%20is%20primarily,the%20reference%20translations%20or%20summaries.)), and compute it for the validation set after each epoch. For this task, it is sufficient to just observe the train loss values.\n",
    "\n",
    "You are already provided the `train_seq2seq_model` function in `utils.translation.train_funcs.py`. You can refer to this file to see the loss function and how a custom training loop with modifications has been implemented. Execute the cell below to train your model.\n",
    "\n",
    "**Note that training will proceed as expected only if the implementation of your model is correct.** You can monitor the training loss to make sure that the model training is proceeding as expected. **Training may take 10-15 minutes depending on the strength of the system.**\n",
    "\n",
    "If you have spare time, feel free to increase the number of epoch gauge if the performance improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "Iter: 0, Loss (iter): 9.109655380249023, Mean Loss (over last 50 iters): 9.109655380249023\n",
      "Iter: 50, Loss (iter): 5.561922073364258, Mean Loss (over last 50 iters): 6.438636302947998\n",
      "Iter: 100, Loss (iter): 5.214922904968262, Mean Loss (over last 50 iters): 5.5021772384643555\n",
      "Iter: 150, Loss (iter): 5.426918029785156, Mean Loss (over last 50 iters): 5.2833685874938965\n",
      "Iter: 200, Loss (iter): 4.98797607421875, Mean Loss (over last 50 iters): 5.141422271728516\n",
      "Iter: 250, Loss (iter): 4.985249042510986, Mean Loss (over last 50 iters): 5.048381805419922\n",
      "Iter: 300, Loss (iter): 4.848669528961182, Mean Loss (over last 50 iters): 4.949271202087402\n",
      "Iter: 350, Loss (iter): 4.706375598907471, Mean Loss (over last 50 iters): 4.819245338439941\n",
      "Iter: 400, Loss (iter): 4.440303802490234, Mean Loss (over last 50 iters): 4.677301406860352\n",
      "Iter: 450, Loss (iter): 4.419108867645264, Mean Loss (over last 50 iters): 4.612499713897705\n",
      "Iter: 500, Loss (iter): 4.456099510192871, Mean Loss (over last 50 iters): 4.542738437652588\n",
      "Iter: 550, Loss (iter): 4.631516933441162, Mean Loss (over last 50 iters): 4.451769828796387\n",
      "Iter: 600, Loss (iter): 4.16755485534668, Mean Loss (over last 50 iters): 4.367379188537598\n",
      "Iter: 650, Loss (iter): 3.981334924697876, Mean Loss (over last 50 iters): 4.332171440124512\n",
      "Iter: 700, Loss (iter): 4.169011116027832, Mean Loss (over last 50 iters): 4.274772644042969\n",
      "Iter: 750, Loss (iter): 4.154201030731201, Mean Loss (over last 50 iters): 4.200132846832275\n",
      "Iter: 800, Loss (iter): 4.123761177062988, Mean Loss (over last 50 iters): 4.110743999481201\n",
      "Iter: 850, Loss (iter): 3.703582763671875, Mean Loss (over last 50 iters): 4.062798976898193\n",
      "Iter: 900, Loss (iter): 3.9847004413604736, Mean Loss (over last 50 iters): 4.002766132354736\n",
      "Iter: 950, Loss (iter): 3.900135040283203, Mean Loss (over last 50 iters): 3.9253525733947754\n",
      "Iter: 1000, Loss (iter): 3.8322181701660156, Mean Loss (over last 50 iters): 3.859828472137451\n",
      "Iter: 1050, Loss (iter): 3.745014190673828, Mean Loss (over last 50 iters): 3.825557231903076\n",
      "Epoch: 2/5\n",
      "Iter: 0, Loss (iter): 4.077396392822266, Mean Loss (over last 50 iters): 3.8189196586608887\n",
      "Iter: 50, Loss (iter): 3.6562752723693848, Mean Loss (over last 50 iters): 3.8229074478149414\n",
      "Iter: 100, Loss (iter): 3.6153454780578613, Mean Loss (over last 50 iters): 3.7657837867736816\n",
      "Iter: 150, Loss (iter): 3.9999465942382812, Mean Loss (over last 50 iters): 3.7360546588897705\n",
      "Iter: 200, Loss (iter): 3.5657718181610107, Mean Loss (over last 50 iters): 3.7040369510650635\n",
      "Iter: 250, Loss (iter): 3.6223669052124023, Mean Loss (over last 50 iters): 3.6549298763275146\n",
      "Iter: 300, Loss (iter): 3.6088674068450928, Mean Loss (over last 50 iters): 3.617948055267334\n",
      "Iter: 350, Loss (iter): 3.4551007747650146, Mean Loss (over last 50 iters): 3.568966865539551\n",
      "Iter: 400, Loss (iter): 3.371131181716919, Mean Loss (over last 50 iters): 3.503650426864624\n",
      "Iter: 450, Loss (iter): 3.310976505279541, Mean Loss (over last 50 iters): 3.4955098628997803\n",
      "Iter: 500, Loss (iter): 3.341442584991455, Mean Loss (over last 50 iters): 3.485941171646118\n",
      "Iter: 550, Loss (iter): 3.574591636657715, Mean Loss (over last 50 iters): 3.4299755096435547\n",
      "Iter: 600, Loss (iter): 3.220203399658203, Mean Loss (over last 50 iters): 3.3799757957458496\n",
      "Iter: 650, Loss (iter): 3.042644500732422, Mean Loss (over last 50 iters): 3.3658761978149414\n",
      "Iter: 700, Loss (iter): 3.1979732513427734, Mean Loss (over last 50 iters): 3.343564510345459\n",
      "Iter: 750, Loss (iter): 3.2429821491241455, Mean Loss (over last 50 iters): 3.2960915565490723\n",
      "Iter: 800, Loss (iter): 3.203420639038086, Mean Loss (over last 50 iters): 3.226780414581299\n",
      "Iter: 850, Loss (iter): 2.9073779582977295, Mean Loss (over last 50 iters): 3.2054150104522705\n",
      "Iter: 900, Loss (iter): 3.221625328063965, Mean Loss (over last 50 iters): 3.1848580837249756\n",
      "Iter: 950, Loss (iter): 3.0351946353912354, Mean Loss (over last 50 iters): 3.134585380554199\n",
      "Iter: 1000, Loss (iter): 3.099609851837158, Mean Loss (over last 50 iters): 3.0992438793182373\n",
      "Iter: 1050, Loss (iter): 3.0444142818450928, Mean Loss (over last 50 iters): 3.086193323135376\n",
      "Epoch: 3/5\n",
      "Iter: 0, Loss (iter): 3.3116369247436523, Mean Loss (over last 50 iters): 3.083157539367676\n",
      "Iter: 50, Loss (iter): 2.9680469036102295, Mean Loss (over last 50 iters): 3.109755754470825\n",
      "Iter: 100, Loss (iter): 2.908003807067871, Mean Loss (over last 50 iters): 3.0718390941619873\n",
      "Iter: 150, Loss (iter): 3.244171380996704, Mean Loss (over last 50 iters): 3.0350430011749268\n",
      "Iter: 200, Loss (iter): 2.9373977184295654, Mean Loss (over last 50 iters): 3.019033908843994\n",
      "Iter: 250, Loss (iter): 2.881625175476074, Mean Loss (over last 50 iters): 2.973245143890381\n",
      "Iter: 300, Loss (iter): 2.9745445251464844, Mean Loss (over last 50 iters): 2.9573798179626465\n",
      "Iter: 350, Loss (iter): 2.867391586303711, Mean Loss (over last 50 iters): 2.9229395389556885\n",
      "Iter: 400, Loss (iter): 2.7574424743652344, Mean Loss (over last 50 iters): 2.8649837970733643\n",
      "Iter: 450, Loss (iter): 2.6409409046173096, Mean Loss (over last 50 iters): 2.863034725189209\n",
      "Iter: 500, Loss (iter): 2.719125270843506, Mean Loss (over last 50 iters): 2.8787097930908203\n",
      "Iter: 550, Loss (iter): 2.8589060306549072, Mean Loss (over last 50 iters): 2.804335355758667\n",
      "Iter: 600, Loss (iter): 2.585202217102051, Mean Loss (over last 50 iters): 2.763356924057007\n",
      "Iter: 650, Loss (iter): 2.4374287128448486, Mean Loss (over last 50 iters): 2.7474582195281982\n",
      "Iter: 700, Loss (iter): 2.593057870864868, Mean Loss (over last 50 iters): 2.7406649589538574\n",
      "Iter: 750, Loss (iter): 2.685563087463379, Mean Loss (over last 50 iters): 2.7097721099853516\n",
      "Iter: 800, Loss (iter): 2.574042320251465, Mean Loss (over last 50 iters): 2.649641752243042\n",
      "Iter: 850, Loss (iter): 2.3903651237487793, Mean Loss (over last 50 iters): 2.633389472961426\n",
      "Iter: 900, Loss (iter): 2.717243194580078, Mean Loss (over last 50 iters): 2.6268460750579834\n",
      "Iter: 950, Loss (iter): 2.4366614818573, Mean Loss (over last 50 iters): 2.574755907058716\n",
      "Iter: 1000, Loss (iter): 2.61752986907959, Mean Loss (over last 50 iters): 2.548496723175049\n",
      "Iter: 1050, Loss (iter): 2.5674331188201904, Mean Loss (over last 50 iters): 2.5467262268066406\n",
      "Epoch: 4/5\n",
      "Iter: 0, Loss (iter): 2.7627058029174805, Mean Loss (over last 50 iters): 2.5404436588287354\n",
      "Iter: 50, Loss (iter): 2.449819564819336, Mean Loss (over last 50 iters): 2.564192533493042\n",
      "Iter: 100, Loss (iter): 2.375563144683838, Mean Loss (over last 50 iters): 2.5371978282928467\n",
      "Iter: 150, Loss (iter): 2.7070119380950928, Mean Loss (over last 50 iters): 2.4884002208709717\n",
      "Iter: 200, Loss (iter): 2.420186996459961, Mean Loss (over last 50 iters): 2.484300374984741\n",
      "Iter: 250, Loss (iter): 2.32969069480896, Mean Loss (over last 50 iters): 2.441744089126587\n",
      "Iter: 300, Loss (iter): 2.4667882919311523, Mean Loss (over last 50 iters): 2.4383678436279297\n",
      "Iter: 350, Loss (iter): 2.4382283687591553, Mean Loss (over last 50 iters): 2.4061975479125977\n",
      "Iter: 400, Loss (iter): 2.2992072105407715, Mean Loss (over last 50 iters): 2.3645758628845215\n",
      "Iter: 450, Loss (iter): 2.1271698474884033, Mean Loss (over last 50 iters): 2.3554887771606445\n",
      "Iter: 500, Loss (iter): 2.224776268005371, Mean Loss (over last 50 iters): 2.3745126724243164\n",
      "Iter: 550, Loss (iter): 2.3283004760742188, Mean Loss (over last 50 iters): 2.300919771194458\n",
      "Iter: 600, Loss (iter): 2.0836730003356934, Mean Loss (over last 50 iters): 2.268390655517578\n",
      "Iter: 650, Loss (iter): 1.973962426185608, Mean Loss (over last 50 iters): 2.2443325519561768\n",
      "Iter: 700, Loss (iter): 2.091383695602417, Mean Loss (over last 50 iters): 2.243778705596924\n",
      "Iter: 750, Loss (iter): 2.188204050064087, Mean Loss (over last 50 iters): 2.2274346351623535\n",
      "Iter: 800, Loss (iter): 2.089571237564087, Mean Loss (over last 50 iters): 2.179011583328247\n",
      "Iter: 850, Loss (iter): 2.009944438934326, Mean Loss (over last 50 iters): 2.1702334880828857\n",
      "Iter: 900, Loss (iter): 2.2623934745788574, Mean Loss (over last 50 iters): 2.1720197200775146\n",
      "Iter: 950, Loss (iter): 1.9846476316452026, Mean Loss (over last 50 iters): 2.1196115016937256\n",
      "Iter: 1000, Loss (iter): 2.159609794616699, Mean Loss (over last 50 iters): 2.106736421585083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1050, Loss (iter): 2.136608600616455, Mean Loss (over last 50 iters): 2.1133933067321777\n",
      "Epoch: 5/5\n",
      "Iter: 0, Loss (iter): 2.31770920753479, Mean Loss (over last 50 iters): 2.1116552352905273\n",
      "Iter: 50, Loss (iter): 2.0536561012268066, Mean Loss (over last 50 iters): 2.11337947845459\n",
      "Iter: 100, Loss (iter): 1.9534906148910522, Mean Loss (over last 50 iters): 2.0997748374938965\n",
      "Iter: 150, Loss (iter): 2.2496261596679688, Mean Loss (over last 50 iters): 2.047212839126587\n",
      "Iter: 200, Loss (iter): 1.979124903678894, Mean Loss (over last 50 iters): 2.0501534938812256\n",
      "Iter: 250, Loss (iter): 1.8822513818740845, Mean Loss (over last 50 iters): 2.012162923812866\n",
      "Iter: 300, Loss (iter): 2.057410478591919, Mean Loss (over last 50 iters): 2.0132553577423096\n",
      "Iter: 350, Loss (iter): 2.055738925933838, Mean Loss (over last 50 iters): 1.986190915107727\n",
      "Iter: 400, Loss (iter): 1.94454026222229, Mean Loss (over last 50 iters): 1.9567152261734009\n",
      "Iter: 450, Loss (iter): 1.735944390296936, Mean Loss (over last 50 iters): 1.9476977586746216\n",
      "Iter: 500, Loss (iter): 1.8381611108779907, Mean Loss (over last 50 iters): 1.9661129713058472\n",
      "Iter: 550, Loss (iter): 1.868006706237793, Mean Loss (over last 50 iters): 1.894250512123108\n",
      "Iter: 600, Loss (iter): 1.653171420097351, Mean Loss (over last 50 iters): 1.8766902685165405\n",
      "Iter: 650, Loss (iter): 1.6038943529129028, Mean Loss (over last 50 iters): 1.8480781316757202\n",
      "Iter: 700, Loss (iter): 1.7274210453033447, Mean Loss (over last 50 iters): 1.8547732830047607\n",
      "Iter: 750, Loss (iter): 1.835981011390686, Mean Loss (over last 50 iters): 1.8452624082565308\n",
      "Iter: 800, Loss (iter): 1.6807645559310913, Mean Loss (over last 50 iters): 1.8090405464172363\n",
      "Iter: 850, Loss (iter): 1.6683293581008911, Mean Loss (over last 50 iters): 1.804696798324585\n",
      "Iter: 900, Loss (iter): 1.9143812656402588, Mean Loss (over last 50 iters): 1.8051855564117432\n",
      "Iter: 950, Loss (iter): 1.6062283515930176, Mean Loss (over last 50 iters): 1.7561523914337158\n",
      "Iter: 1000, Loss (iter): 1.842579960823059, Mean Loss (over last 50 iters): 1.7574464082717896\n",
      "Iter: 1050, Loss (iter): 1.7953439950942993, Mean Loss (over last 50 iters): 1.7709031105041504\n"
     ]
    }
   ],
   "source": [
    "from utils.translation.train_funcs import train_seq2seq_model\n",
    "\n",
    "# Train the model. Use the Adam optimizer with 1e-3 learning rate.\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "train_seq2seq_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    num_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluating Results (2%)\n",
    "\n",
    "Our training function only shows the training loss value. To assess the performance of the model, we can perform some predictions and decode the input/output sentences. \n",
    "\n",
    "<font color=\"red\"><strong>TODO:</strong></font> Run the following cells to qualitatively asses the quality of the generated sentences and the performance of the trained model.\n",
    "\n",
    "**NOTE**: As we are dealing with a generation task, the outputs will vary depending on the final trained model. Therefore, we have provided a set of example outputs with the translation quality you can expect from the trained model. You results may be different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these cells to evaluate your model on one batch of the validation set\n",
    "val_sample = val_loader.shuffle(10000).take(1)\n",
    "val_sample = next(iter(val_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1\n",
      "Dutch Sentence:  vanwege haar hulp verbeterde mijn frans beetje bij beetje\n",
      "English Sentence (Truth):  with her help my french improved little by little\n",
      "English Sentence (Pred) mom and i have to speak english with my parents\n",
      "Sample: 2\n",
      "Dutch Sentence:  morgen blijf ik thuis\n",
      "English Sentence (Truth):  i ll stay home tomorrow\n",
      "English Sentence (Pred) i ll stay at home tomorrow\n",
      "Sample: 3\n",
      "Dutch Sentence:  ik maak een sneeuwpop\n",
      "English Sentence (Truth):  i m making a snowman\n",
      "English Sentence (Pred) i m a [UNK]\n",
      "Sample: 4\n",
      "Dutch Sentence:  ze hebben me verteld om hier niet te komen\n",
      "English Sentence (Truth):  i was told not to come here\n",
      "English Sentence (Pred) they told me not to be here\n",
      "Sample: 5\n",
      "Dutch Sentence:  ze stond voor de klas\n",
      "English Sentence (Truth):  she used to be a teacher\n",
      "English Sentence (Pred) she was in the hospital\n",
      "Sample: 6\n",
      "Dutch Sentence:  ik heb geprobeerd uw kantoor te bellen\n",
      "English Sentence (Truth):  i tried calling your office\n",
      "English Sentence (Pred) i ve never seen you to go\n",
      "Sample: 7\n",
      "Dutch Sentence:  tom heeft een blauw oog\n",
      "English Sentence (Truth):  tom has a black eye\n",
      "English Sentence (Pred) tom has a beautiful coat\n",
      "Sample: 8\n",
      "Dutch Sentence:  dat ga ik nooit doen\n",
      "English Sentence (Truth):  i m not ever going to do that\n",
      "English Sentence (Pred) i m never doing that\n",
      "Sample: 9\n",
      "Dutch Sentence:  ik kan uw familienaam niet uitspreken\n",
      "English Sentence (Truth):  i can t pronounce your last name\n",
      "English Sentence (Pred) i can t my answer for me\n",
      "Sample: 10\n",
      "Dutch Sentence:  iedereen haat tom\n",
      "English Sentence (Truth):  everybody hates tom\n",
      "English Sentence (Pred) everyone hates tom\n",
      "Sample: 11\n",
      "Dutch Sentence:  tom heeft de kluis geopend\n",
      "English Sentence (Truth):  tom opened the safe\n",
      "English Sentence (Pred) tom started tired\n"
     ]
    }
   ],
   "source": [
    "val_inp, val_target = val_sample\n",
    "decoded_inputs = []\n",
    "reserved_tokens = ['[PAD]', '[SOS]', '[EOS]']\n",
    "for inp in val_inp.numpy():\n",
    "    decoded_text = decode_text(inp, vocab=nl_vocab)\n",
    "    decoded_inputs.append([token for token in decoded_text if token not in reserved_tokens])\n",
    "\n",
    "val_pred = model(val_inp, training=False)\n",
    "decoded_outputs = model.decode_tokens(val_pred)\n",
    "decoded_ground_truth = model.decode_tokens(val_target[:, 1:])\n",
    "\n",
    "samples_to_show = 10 #Should be <= batch size\n",
    "\n",
    "for i, decoded_data in enumerate(zip(decoded_inputs, decoded_ground_truth, decoded_outputs)):\n",
    "    nl_sentence = ' '.join(decoded_data[0])\n",
    "    gt_en_sentence = ' '.join(decoded_data[1])\n",
    "    pred_en_sentence = ' '.join(decoded_data[2])\n",
    "    print('Sample:', i+1)\n",
    "    print('Dutch Sentence: ', nl_sentence)\n",
    "    print('English Sentence (Truth): ', gt_en_sentence)\n",
    "    print('English Sentence (Pred)', pred_en_sentence)\n",
    "    if i > samples_to_show - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Expected Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample: 1\n",
    "Dutch Sentence:  waar hebben jullie het verstopt ?\n",
    "English Sentence (Truth):  where did you hide it ?\n",
    "English Sentence (Pred) where did you do it ?\n",
    "Sample: 2\n",
    "Dutch Sentence:  denk jij dit ?\n",
    "English Sentence (Truth):  is that what you think ?\n",
    "English Sentence (Pred) do you think this ?\n",
    "Sample: 3\n",
    "Dutch Sentence:  de treinen rijden s nachts minder vaak\n",
    "English Sentence (Truth):  the trains don t run as often at night\n",
    "English Sentence (Pred) the [UNK] [UNK] to [UNK] a week\n",
    "Sample: 4\n",
    "Dutch Sentence:  ik weet hoe dit werkt\n",
    "English Sentence (Truth):  i know how this works\n",
    "English Sentence (Pred) i know that this dictionary\n",
    "Sample: 5\n",
    "Dutch Sentence:  houd je toespraak kort\n",
    "English Sentence (Truth):  keep your speech short\n",
    "English Sentence (Pred) look at your country\n",
    "Sample: 6\n",
    "Dutch Sentence:  help me alsjeblieft een trui uit te kiezen die bij mijn nieuwe jurk past\n",
    "English Sentence (Truth):  please help me pick out a sweater which matches my new dress\n",
    "English Sentence (Pred) please give me a new dictionary for me this year ago\n",
    "Sample: 7\n",
    "Dutch Sentence:  welk jaar is het ?\n",
    "English Sentence (Truth):  what year is it ?\n",
    "English Sentence (Pred) what is the last ?\n",
    "Sample: 8\n",
    "Dutch Sentence:  welk verschil is er tussen dit en dat ?\n",
    "English Sentence (Truth):  what is the difference between this and that ?\n",
    "English Sentence (Pred) what s the difference between this bird ?\n",
    "Sample: 9\n",
    "Dutch Sentence:  is dat onze bus ?\n",
    "English Sentence (Truth):  is that our bus ?\n",
    "English Sentence (Pred) is this the book ?\n",
    "Sample: 10\n",
    "Dutch Sentence:  kun je me vannacht een [UNK] doen en op mijn kinderen oppassen ?\n",
    "English Sentence (Truth):  could you do me a favor and [UNK] my kids tonight ?\n",
    "English Sentence (Pred) can you please tell the truth to me a doctor ?\n",
    "Sample: 11\n",
    "Dutch Sentence:  ik bleef daar\n",
    "English Sentence (Truth):  i stayed there\n",
    "English Sentence (Pred) i felt [UNK]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"><strong>TODO:</strong></font> <b>Answer the following questions:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Describe your observations of the model's evaluation performance. Briefly explain any one method to improve the model architecture based on the lecture readings, or online sources.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "The model shows mixed accuracy in translating Dutch to English, effectively handling some sentences but struggling with complex structures and specific vocabulary. Notable are the errors and misinterpretations in several samples, indicating difficulties in capturing nuances and dealing with unknown words. To improve, integrating an attention mechanism is advisable. This approach helps the model focus on relevant parts of the input for each word in the output, particularly beneficial for longer sentences. Implementing attention requires adding a specialized layer to the neural network and adjusting both encoder and decoder components. Overall, while capable of basic translations, the model needs enhancements, particularly in handling complex language elements, which attention mechanisms can address.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **During the data preprocessing, we encoded each word in the input/target language to a number based on the vocabulary. This is known as tokenization. Briefly explain any one other method of tokenization, and why it might be beneficial to this particular task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "Byte Pair Encoding (BPE): A popular subword tokenization method, initially used for data compression, that iteratively merges frequent character pairs to form common subwords.\n",
    "\n",
    "Byte Pair Encoding (BPE) is highly beneficial for machine translation as it efficiently manages rare and unknown words by breaking them into known subword units. This approach allows models to handle unfamiliar vocabulary, crucial in translation tasks. BPE balances vocabulary size with language diversity, accommodating inflected languages by representing various word forms without needing an exhaustive vocabulary. It effectively integrates loanwords, proper nouns, and technical terms by breaking them into processable subwords. Additionally, BPE's ability to uncover common subwords across languages enhances translation quality, especially for linguistically related languages, thereby improving the model's overall flexibility and robustness in handling natural language nuances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a simple LSTM-based seq2seq model. The performance may not be the best, since Dutch and English are naturally complex languages. The state-of-the-art translation models are based on Transformer Networks that use the attention mechanism. (further reading: https://nlpprogress.com/english/machine_translation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) Part 6: Bidirectional LSTM (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple modification that we can do to significantly improve the quality of the generated sentences is to change the Encoder LSTM to be bidirectional. This will improve the performance because different languages tend to have different sentence structures, and in the case of Dutch, crucial information for a given word may not be available until later on in the sentence. \n",
    "\n",
    "**SIDE NOTE**: Changing the decoder to be bidirectional will not work in a text generation task (in our case, translation). Feel free to think of why this is the case. (No written is required).\n",
    "\n",
    "<font color=\"red\"><strong>BONUS TODO:</strong></font> <b> Complete the BidirectionalEncoder Class in utils/translation/layers.py, and run the training and validation loops to compare the generated translations with the previous model you implemented.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.translation.layers import TranslationModel\n",
    "\n",
    "eng_vocab_size = len(eng_vocab)\n",
    "nl_vocab_size = len(nl_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "# Initialize Model with bidirectional_encoder = True\n",
    "# NOTE the bidirectional_encoder = True\n",
    "model = TranslationModel(nl_vocab_size, eng_vocab_size, hidden_size, eng_vocab, bidirectional_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "Iter: 0, Loss (iter): 9.109925270080566, Mean Loss (over last 50 iters): 9.109925270080566\n",
      "Iter: 50, Loss (iter): 5.590933799743652, Mean Loss (over last 50 iters): 6.434598445892334\n",
      "Iter: 100, Loss (iter): 5.217302322387695, Mean Loss (over last 50 iters): 5.515486240386963\n",
      "Iter: 150, Loss (iter): 5.433877468109131, Mean Loss (over last 50 iters): 5.275567054748535\n",
      "Iter: 200, Loss (iter): 4.962477684020996, Mean Loss (over last 50 iters): 5.11947774887085\n",
      "Iter: 250, Loss (iter): 4.911623954772949, Mean Loss (over last 50 iters): 4.987344741821289\n",
      "Iter: 300, Loss (iter): 4.743401527404785, Mean Loss (over last 50 iters): 4.850338935852051\n",
      "Iter: 350, Loss (iter): 4.550870895385742, Mean Loss (over last 50 iters): 4.684281826019287\n",
      "Iter: 400, Loss (iter): 4.284757137298584, Mean Loss (over last 50 iters): 4.5463056564331055\n",
      "Iter: 450, Loss (iter): 4.201324462890625, Mean Loss (over last 50 iters): 4.4350690841674805\n",
      "Iter: 500, Loss (iter): 4.21593713760376, Mean Loss (over last 50 iters): 4.330495834350586\n",
      "Iter: 550, Loss (iter): 4.3549017906188965, Mean Loss (over last 50 iters): 4.206145763397217\n",
      "Iter: 600, Loss (iter): 3.8962788581848145, Mean Loss (over last 50 iters): 4.102331161499023\n",
      "Iter: 650, Loss (iter): 3.7451882362365723, Mean Loss (over last 50 iters): 4.0530476570129395\n",
      "Iter: 700, Loss (iter): 3.8616209030151367, Mean Loss (over last 50 iters): 3.9777824878692627\n",
      "Iter: 750, Loss (iter): 3.788447856903076, Mean Loss (over last 50 iters): 3.894730567932129\n",
      "Iter: 800, Loss (iter): 3.7887728214263916, Mean Loss (over last 50 iters): 3.7913312911987305\n",
      "Iter: 850, Loss (iter): 3.353327512741089, Mean Loss (over last 50 iters): 3.725139856338501\n",
      "Iter: 900, Loss (iter): 3.6896965503692627, Mean Loss (over last 50 iters): 3.686042070388794\n",
      "Iter: 950, Loss (iter): 3.5541083812713623, Mean Loss (over last 50 iters): 3.609321355819702\n",
      "Iter: 1000, Loss (iter): 3.5352847576141357, Mean Loss (over last 50 iters): 3.5507333278656006\n",
      "Iter: 1050, Loss (iter): 3.4210662841796875, Mean Loss (over last 50 iters): 3.507559299468994\n",
      "Epoch: 2/5\n",
      "Iter: 0, Loss (iter): 3.767141103744507, Mean Loss (over last 50 iters): 3.5007824897766113\n",
      "Iter: 50, Loss (iter): 3.3184566497802734, Mean Loss (over last 50 iters): 3.5024523735046387\n",
      "Iter: 100, Loss (iter): 3.2610509395599365, Mean Loss (over last 50 iters): 3.4475409984588623\n",
      "Iter: 150, Loss (iter): 3.6885793209075928, Mean Loss (over last 50 iters): 3.4064366817474365\n",
      "Iter: 200, Loss (iter): 3.2043800354003906, Mean Loss (over last 50 iters): 3.3638744354248047\n",
      "Iter: 250, Loss (iter): 3.3059353828430176, Mean Loss (over last 50 iters): 3.3169562816619873\n",
      "Iter: 300, Loss (iter): 3.264509677886963, Mean Loss (over last 50 iters): 3.2783286571502686\n",
      "Iter: 350, Loss (iter): 3.1872570514678955, Mean Loss (over last 50 iters): 3.2294440269470215\n",
      "Iter: 400, Loss (iter): 3.007115364074707, Mean Loss (over last 50 iters): 3.1540024280548096\n",
      "Iter: 450, Loss (iter): 2.956465721130371, Mean Loss (over last 50 iters): 3.1622421741485596\n",
      "Iter: 500, Loss (iter): 3.0403926372528076, Mean Loss (over last 50 iters): 3.159409999847412\n",
      "Iter: 550, Loss (iter): 3.17846417427063, Mean Loss (over last 50 iters): 3.0699973106384277\n",
      "Iter: 600, Loss (iter): 2.832517147064209, Mean Loss (over last 50 iters): 3.0298995971679688\n",
      "Iter: 650, Loss (iter): 2.6811630725860596, Mean Loss (over last 50 iters): 3.006709575653076\n",
      "Iter: 700, Loss (iter): 2.86255145072937, Mean Loss (over last 50 iters): 2.984405517578125\n",
      "Iter: 750, Loss (iter): 2.8584022521972656, Mean Loss (over last 50 iters): 2.9341390132904053\n",
      "Iter: 800, Loss (iter): 2.80309796333313, Mean Loss (over last 50 iters): 2.8652849197387695\n",
      "Iter: 850, Loss (iter): 2.5537288188934326, Mean Loss (over last 50 iters): 2.8351361751556396\n",
      "Iter: 900, Loss (iter): 2.92639422416687, Mean Loss (over last 50 iters): 2.828190326690674\n",
      "Iter: 950, Loss (iter): 2.6959354877471924, Mean Loss (over last 50 iters): 2.7679553031921387\n",
      "Iter: 1000, Loss (iter): 2.727738380432129, Mean Loss (over last 50 iters): 2.7363903522491455\n",
      "Iter: 1050, Loss (iter): 2.720233917236328, Mean Loss (over last 50 iters): 2.7180347442626953\n",
      "Epoch: 3/5\n",
      "Iter: 0, Loss (iter): 2.9682395458221436, Mean Loss (over last 50 iters): 2.714266061782837\n",
      "Iter: 50, Loss (iter): 2.5846574306488037, Mean Loss (over last 50 iters): 2.7381482124328613\n",
      "Iter: 100, Loss (iter): 2.5316262245178223, Mean Loss (over last 50 iters): 2.7116451263427734\n",
      "Iter: 150, Loss (iter): 2.9118669033050537, Mean Loss (over last 50 iters): 2.6709351539611816\n",
      "Iter: 200, Loss (iter): 2.576801061630249, Mean Loss (over last 50 iters): 2.6502628326416016\n",
      "Iter: 250, Loss (iter): 2.5417261123657227, Mean Loss (over last 50 iters): 2.617765426635742\n",
      "Iter: 300, Loss (iter): 2.629798173904419, Mean Loss (over last 50 iters): 2.5909600257873535\n",
      "Iter: 350, Loss (iter): 2.5859482288360596, Mean Loss (over last 50 iters): 2.5611557960510254\n",
      "Iter: 400, Loss (iter): 2.39237904548645, Mean Loss (over last 50 iters): 2.5032145977020264\n",
      "Iter: 450, Loss (iter): 2.2922427654266357, Mean Loss (over last 50 iters): 2.506768226623535\n",
      "Iter: 500, Loss (iter): 2.4006075859069824, Mean Loss (over last 50 iters): 2.5235483646392822\n",
      "Iter: 550, Loss (iter): 2.5281972885131836, Mean Loss (over last 50 iters): 2.4338619709014893\n",
      "Iter: 600, Loss (iter): 2.2137861251831055, Mean Loss (over last 50 iters): 2.4083123207092285\n",
      "Iter: 650, Loss (iter): 2.0925800800323486, Mean Loss (over last 50 iters): 2.391505479812622\n",
      "Iter: 700, Loss (iter): 2.2585484981536865, Mean Loss (over last 50 iters): 2.388659715652466\n",
      "Iter: 750, Loss (iter): 2.298917531967163, Mean Loss (over last 50 iters): 2.3571131229400635\n",
      "Iter: 800, Loss (iter): 2.1949450969696045, Mean Loss (over last 50 iters): 2.3034894466400146\n",
      "Iter: 850, Loss (iter): 2.046926736831665, Mean Loss (over last 50 iters): 2.2872440814971924\n",
      "Iter: 900, Loss (iter): 2.385396957397461, Mean Loss (over last 50 iters): 2.282198429107666\n",
      "Iter: 950, Loss (iter): 2.1486330032348633, Mean Loss (over last 50 iters): 2.233680009841919\n",
      "Iter: 1000, Loss (iter): 2.2419114112854004, Mean Loss (over last 50 iters): 2.213484287261963\n",
      "Iter: 1050, Loss (iter): 2.2706241607666016, Mean Loss (over last 50 iters): 2.2131614685058594\n",
      "Epoch: 4/5\n",
      "Iter: 0, Loss (iter): 2.46675705909729, Mean Loss (over last 50 iters): 2.2123639583587646\n",
      "Iter: 50, Loss (iter): 2.0958282947540283, Mean Loss (over last 50 iters): 2.227567195892334\n",
      "Iter: 100, Loss (iter): 2.0406112670898438, Mean Loss (over last 50 iters): 2.2111897468566895\n",
      "Iter: 150, Loss (iter): 2.382856845855713, Mean Loss (over last 50 iters): 2.1729636192321777\n",
      "Iter: 200, Loss (iter): 2.144045829772949, Mean Loss (over last 50 iters): 2.1596627235412598\n",
      "Iter: 250, Loss (iter): 1.9996297359466553, Mean Loss (over last 50 iters): 2.125852346420288\n",
      "Iter: 300, Loss (iter): 2.213611364364624, Mean Loss (over last 50 iters): 2.1164345741271973\n",
      "Iter: 350, Loss (iter): 2.182405471801758, Mean Loss (over last 50 iters): 2.0922303199768066\n",
      "Iter: 400, Loss (iter): 1.9611718654632568, Mean Loss (over last 50 iters): 2.0483827590942383\n",
      "Iter: 450, Loss (iter): 1.8546972274780273, Mean Loss (over last 50 iters): 2.0533502101898193\n",
      "Iter: 500, Loss (iter): 1.9632046222686768, Mean Loss (over last 50 iters): 2.071140766143799\n",
      "Iter: 550, Loss (iter): 2.059871196746826, Mean Loss (over last 50 iters): 1.990317940711975\n",
      "Iter: 600, Loss (iter): 1.7734562158584595, Mean Loss (over last 50 iters): 1.9739471673965454\n",
      "Iter: 650, Loss (iter): 1.7002145051956177, Mean Loss (over last 50 iters): 1.9551044702529907\n",
      "Iter: 700, Loss (iter): 1.8170254230499268, Mean Loss (over last 50 iters): 1.9550763368606567\n",
      "Iter: 750, Loss (iter): 1.8993810415267944, Mean Loss (over last 50 iters): 1.9391826391220093\n",
      "Iter: 800, Loss (iter): 1.79864501953125, Mean Loss (over last 50 iters): 1.8963768482208252\n",
      "Iter: 850, Loss (iter): 1.7064954042434692, Mean Loss (over last 50 iters): 1.8917406797409058\n",
      "Iter: 900, Loss (iter): 1.9954248666763306, Mean Loss (over last 50 iters): 1.8913341760635376\n",
      "Iter: 950, Loss (iter): 1.7360798120498657, Mean Loss (over last 50 iters): 1.849845290184021\n",
      "Iter: 1000, Loss (iter): 1.8709218502044678, Mean Loss (over last 50 iters): 1.8420116901397705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1050, Loss (iter): 1.9437633752822876, Mean Loss (over last 50 iters): 1.8428921699523926\n",
      "Epoch: 5/5\n",
      "Iter: 0, Loss (iter): 2.0554192066192627, Mean Loss (over last 50 iters): 1.846621036529541\n",
      "Iter: 50, Loss (iter): 1.7325129508972168, Mean Loss (over last 50 iters): 1.8505859375\n",
      "Iter: 100, Loss (iter): 1.6618834733963013, Mean Loss (over last 50 iters): 1.8362765312194824\n",
      "Iter: 150, Loss (iter): 1.9841867685317993, Mean Loss (over last 50 iters): 1.7974162101745605\n",
      "Iter: 200, Loss (iter): 1.8311561346054077, Mean Loss (over last 50 iters): 1.7903605699539185\n",
      "Iter: 250, Loss (iter): 1.628451943397522, Mean Loss (over last 50 iters): 1.7640877962112427\n",
      "Iter: 300, Loss (iter): 1.8788560628890991, Mean Loss (over last 50 iters): 1.7517120838165283\n",
      "Iter: 350, Loss (iter): 1.8563867807388306, Mean Loss (over last 50 iters): 1.742863416671753\n",
      "Iter: 400, Loss (iter): 1.648897409439087, Mean Loss (over last 50 iters): 1.7100034952163696\n",
      "Iter: 450, Loss (iter): 1.5147981643676758, Mean Loss (over last 50 iters): 1.710227370262146\n",
      "Iter: 500, Loss (iter): 1.6337676048278809, Mean Loss (over last 50 iters): 1.7291620969772339\n",
      "Iter: 550, Loss (iter): 1.7126954793930054, Mean Loss (over last 50 iters): 1.663421630859375\n",
      "Iter: 600, Loss (iter): 1.4552206993103027, Mean Loss (over last 50 iters): 1.6506603956222534\n",
      "Iter: 650, Loss (iter): 1.460714340209961, Mean Loss (over last 50 iters): 1.6267989873886108\n",
      "Iter: 700, Loss (iter): 1.4615579843521118, Mean Loss (over last 50 iters): 1.6333763599395752\n",
      "Iter: 750, Loss (iter): 1.5920943021774292, Mean Loss (over last 50 iters): 1.6252702474594116\n",
      "Iter: 800, Loss (iter): 1.5052703619003296, Mean Loss (over last 50 iters): 1.5922253131866455\n",
      "Iter: 850, Loss (iter): 1.4568886756896973, Mean Loss (over last 50 iters): 1.5925618410110474\n",
      "Iter: 900, Loss (iter): 1.7022696733474731, Mean Loss (over last 50 iters): 1.5933645963668823\n",
      "Iter: 950, Loss (iter): 1.4166992902755737, Mean Loss (over last 50 iters): 1.5580759048461914\n",
      "Iter: 1000, Loss (iter): 1.5990233421325684, Mean Loss (over last 50 iters): 1.5602576732635498\n",
      "Iter: 1050, Loss (iter): 1.655261516571045, Mean Loss (over last 50 iters): 1.556834101676941\n"
     ]
    }
   ],
   "source": [
    "from utils.translation.train_funcs import train_seq2seq_model\n",
    "\n",
    "# Train the model. Use the Adam optimizer with 1e-3 learning rate.\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "train_seq2seq_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these cells to evaluate your model on one batch of the validation set\n",
    "val_sample = val_loader.shuffle(10000).take(1)\n",
    "val_sample = next(iter(val_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1\n",
      "Dutch Sentence:  ik heb ook in boston gewoond\n",
      "English Sentence (Truth):  i ve also lived in boston\n",
      "English Sentence (Pred) i lived in boston\n",
      "Sample: 2\n",
      "Dutch Sentence:  of je het nu leuk vindt of niet je moet gaan\n",
      "English Sentence (Truth):  like it or not you have to go\n",
      "English Sentence (Pred) you don t feel better to do that\n",
      "Sample: 3\n",
      "Dutch Sentence:  hij hoeft het niet te weten\n",
      "English Sentence (Truth):  he doesn t have to know\n",
      "English Sentence (Pred) he doesn t want to do it\n",
      "Sample: 4\n",
      "Dutch Sentence:  tom checkte zijn e mails\n",
      "English Sentence (Truth):  tom checked his emails\n",
      "English Sentence (Pred) tom checked his email\n",
      "Sample: 5\n",
      "Dutch Sentence:  het was dringend\n",
      "English Sentence (Truth):  it was urgent\n",
      "English Sentence (Pred) it was snowing\n",
      "Sample: 6\n",
      "Dutch Sentence:  wat fijn om je weer te zien\n",
      "English Sentence (Truth):  how nice to see you again\n",
      "English Sentence (Pred) what kind of things do you like to see\n",
      "Sample: 7\n",
      "Dutch Sentence:  je kunt nu beter weggaan\n",
      "English Sentence (Truth):  you may as well leave now\n",
      "English Sentence (Pred) you can better work\n",
      "Sample: 8\n",
      "Dutch Sentence:  wanneer was de laatste keer dat je iemand hebt omhelsd ?\n",
      "English Sentence (Truth):  when was the last time you hugged someone ?\n",
      "English Sentence (Pred) when was the last time you saw tom ?\n",
      "Sample: 9\n",
      "Dutch Sentence:  wat zoudt gij doen in mijn plaats ?\n",
      "English Sentence (Truth):  if you were in my place what would you do ?\n",
      "English Sentence (Pred) what do you want to do in my place ?\n",
      "Sample: 10\n",
      "Dutch Sentence:  is er iets gebeurd vandaag ?\n",
      "English Sentence (Truth):  did something happen today ?\n",
      "English Sentence (Pred) is it happened here ?\n",
      "Sample: 11\n",
      "Dutch Sentence:  tom hoeft dat niet te weten\n",
      "English Sentence (Truth):  tom doesn t need to know that\n",
      "English Sentence (Pred) tom doesn t need to know\n"
     ]
    }
   ],
   "source": [
    "val_inp, val_target = val_sample\n",
    "decoded_inputs = []\n",
    "reserved_tokens = ['[PAD]', '[SOS]', '[EOS]']\n",
    "for inp in val_inp.numpy():\n",
    "    decoded_text = decode_text(inp, vocab=nl_vocab)\n",
    "    decoded_inputs.append([token for token in decoded_text if token not in reserved_tokens])\n",
    "\n",
    "val_pred = model(val_inp, training=False)\n",
    "decoded_outputs = model.decode_tokens(val_pred)\n",
    "decoded_ground_truth = model.decode_tokens(val_target[:, 1:])\n",
    "\n",
    "samples_to_show = 10 #Should be <= batch size\n",
    "\n",
    "for i, decoded_data in enumerate(zip(decoded_inputs, decoded_ground_truth, decoded_outputs)):\n",
    "    nl_sentence = ' '.join(decoded_data[0])\n",
    "    gt_en_sentence = ' '.join(decoded_data[1])\n",
    "    pred_en_sentence = ' '.join(decoded_data[2])\n",
    "    print('Sample:', i+1)\n",
    "    print('Dutch Sentence: ', nl_sentence)\n",
    "    print('English Sentence (Truth): ', gt_en_sentence)\n",
    "    print('English Sentence (Pred)', pred_en_sentence)\n",
    "    if i > samples_to_show - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>BONUS TODO:</strong></font> <b> Briefly describe the differences in quality of the translations between the model with and without a bidirectional encoder. (1-2 sentenes is enough) <b/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "The model with a bidirectional encoder generally produces higher quality translations compared to the model without it, as the bidirectional encoder captures contextual information from both directions of the input sequence, leading to a more nuanced and accurate understanding of the text. This results in translations that are more coherent and contextually appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Assignment_3_task_2_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 1: XOR** (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print('Using GPU(0):', physical_devices[0])\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reload code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1: Backpropagation Through Time (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider some input data $x_1, x_2$ and ground truth $p_1, p_2$ (the indices denote different time steps), a simple RNN network is shown in the following figure. \n",
    "\n",
    "![bptt](./img/bptt2.jpg)\n",
    "\n",
    "Here, $w_x, w_h, b_1, w, b_2 \\in R$ are scalar parameters. The loss function is **Mean Squared Error (MSE)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO:</strong></font>\n",
    "\n",
    "Assume the input $(x_1, x_2) = (1, -1)$, ground truth $(p_1, p_2) = (0, 1)$, $h_0 = 0$, and $(w_x, w_h, b_1, w, b_2) = (1, -2, 3, 2, 1)$, derive both the forward and backward pass (keep $4$ digits after the decimal). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>Hint:</strong></font> The sigmoid function and its derivative\n",
    "\n",
    "$$\n",
    "\\sigma (x) = \\frac{1}{1 + e^{-x}}, \\quad\n",
    "\\nabla_x \\sigma (x) = \\frac{e^{-x}}{1 + e^{-x}} = \\sigma (x) (1 - \\sigma (x))\n",
    "$$\n",
    "\n",
    "- Derive the equations and intermediate variables first before plugging in values, don't just fill in the answers\n",
    "- Use LaTeX-style equations\n",
    "- Your derivations don't need to be wrapped with special color :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>Asnwer:</strong></font>\n",
    "\n",
    "$$\\text{Your Derivations Here.}$$\n",
    "\n",
    "- The gradients\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\nabla_{w_x} L = \\color{cyan}{[nu.]} \\\\\n",
    "\\nabla_{w_h} L = \\color{cyan}{[nu.]} \\\\\n",
    "\\nabla_{b_1} L = \\color{cyan}{[nu.]} \\\\\n",
    "\\nabla_{w} L = \\color{cyan}{[nu.]} \\\\\n",
    "\\nabla_{b_2} L = \\color{cyan}{[nu.]} \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use TensorFlow modules to create XOR network (15%)\n",
    "\n",
    "In this part, you need to build and train an XOR network that can learn the XOR function. It is a very simple implementation of RNN and will give you an idea of how RNN is built and how to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR network\n",
    "\n",
    "![xnor_net](./img/xnor.png)\n",
    "\n",
    "XOR network can learn the XOR ($\\oplus$) function. If the input is\n",
    "\n",
    "$$\n",
    "(x_0, ..., x_7) = (0, 0, 1, 1, 1, 1, 1, 0)\n",
    "$$\n",
    "\n",
    "Then, the output should be\n",
    "\n",
    "$$\n",
    "(y_0, ..., y_7) = (0, 0, 1, 0, 1, 0, 1, 1)\n",
    "$$\n",
    "\n",
    "That is, \n",
    "\n",
    "$$\n",
    "y_n = x_0 \\oplus x_1 \\oplus ... \\oplus x_{n-1} \\oplus x_{n}\n",
    "$$\n",
    "\n",
    "It is also interesting to realize that the equation effectively equivalates to \n",
    "\n",
    "$$\n",
    "y_i = \\sum_{j = 0}^i x_j \\mod 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset\n",
    "This function provides a way to generate the data that is needed for the training process. You should utilize it when building your training function for the GRU. Read the source code for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import create_xor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_xor_dataset(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Network using a TensorFlow LSTMCell and GRUCell\n",
    "\n",
    "In this section, you are asked to build an XOR network using a TensorFlow LSTMCell and a GRUCell. In TensorFlow 2, these two cells are supported by Keras. Please check the online documents below.\n",
    "\n",
    "Use TensorFlow to build and train your XOR network. The dataset is already provided. You will do the following:\n",
    "- Learn how to use `tf.keras.layers.LSTM` and `tf.keras.layers.GRU` in TensorFlow(Keras). \n",
    "- Choose appropriate parameters to build a model (Sequential Model in Keras is suggested). \n",
    "- Compile your model with appropriate loss function, optimizer, metrics, etc.\n",
    "- Train your model and see the loss history.\n",
    "\n",
    "Tips: \n",
    "1. Make sure that the shape of your data is correct after every step.\n",
    "2. Choose your loss function according to your network design.\n",
    "3. Choose 'accuracy' as your metric when compiling your model.\n",
    "4. Make sure that the names of the history for the network with LSTMCell and GRUCell (which you used while training) are the same as the ones in the plotting functions.\n",
    "4. Feel free to consult the TAs if you get stuck somewhere.\n",
    "\n",
    "Reference: \n",
    "1. [TensorFlow(Keras): Working with RNNs](https://keras.io/guides/working_with_rnns/)\n",
    "2. [TensorFlow: Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn)\n",
    "3. [TensorFlow LSTM Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "4. [TensorFlow GRU Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "5. [TensorFlow: Sequential Model](https://www.tensorflow.org/guide/keras/sequential_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Network with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "in_data, out_data = create_xor_dataset(1000) # Create a dataset with a batch size of 1000\n",
    "print('Input data:', in_data.shape, in_data.dtype)\n",
    "print('Labels:', out_data.shape, out_data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting RNN Inputs\n",
    "\n",
    "Typically, the input data of an RNN has the form of\n",
    "\n",
    "$$\n",
    "X \\in R^{T \\times N \\times D}\n",
    "$$\n",
    "\n",
    "where $T$ is ***the number of time steps***, $N$ is the batch size and $D$ is the dimension of each input. \n",
    "\n",
    "At each time step $t$, we feed\n",
    "\n",
    "$$\n",
    "X_t \\in R^{N \\times D}\n",
    "$$\n",
    "\n",
    "into the RNN to generate an output (i.e. the hidden state) $h_t \\in R^{N \\times K}$. \n",
    "\n",
    "Some tasks only take interest in the final output $h_T$, while others need the output from all of the time steps as \n",
    "\n",
    "$$\n",
    "H = [h_1, \\dots, h_T] \\in R^{T \\times N \\times K}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE:</strong></font>\n",
    "\n",
    "In real-life RNN models, the orders of $T$ and $N$ are actually exchangeable by specifying certain model parameters, which are `time_major` in Tensorflow and `batch_first` in Pytorch. \n",
    "\n",
    "Use whatever you like in your implementations, but make sure to have a consistent input shape. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Answer the question: \n",
    "\n",
    "We talked about two usages of an RNN model: \n",
    "\n",
    "- Return only $h_T$\n",
    "- Return all $H = [h_1, \\dots, h_T]$\n",
    "\n",
    "In the context of our XOR function, which one should we use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "\n",
    "$$\\text{Write your answer here.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>HINT:</strong></font>\n",
    "\n",
    "Upon answering the question above, think about how this is done with Tensorflow. Please reference [`LSTM` documents](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint for creating a model with LSTM\n",
    "\n",
    "```\n",
    "model = tf.keras.Sequential() \n",
    "model.add([LAYER1])\n",
    "model.add([LAYER2])\n",
    "model.add([...])\n",
    "model.add([OUTPUT_LAYER])\n",
    "\n",
    "model.summary() \n",
    "model.compile(loss=[LOSS_FUNCTION], optimizer=[OPTIMIZER], metrics=['accuracy'])\n",
    "\n",
    "history_LSTM = model.fit(in_data, out_data, batch_size=64, epochs=15) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Build a LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# TODO: Build a network with LSTM and train it    #\n",
    "#                                                 #\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Network with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "in_data, out_data = create_xor_dataset(1000) # Create a dataset with a batch size of 1000\n",
    "print('Input data:', in_data.shape, in_data.dtype)\n",
    "print('Labels:', out_data.shape, out_data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Build a GRU Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# TODO: Build a model with GRU and train it.      #\n",
    "#                                                 #\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_LSTM.history['loss'], label='LSTM')\n",
    "plt.plot(history_GRU.history['loss'], label='GRU')\n",
    "plt.title('LSTM/GRU loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_LSTM.history['accuracy'], label='LSTM')\n",
    "plt.plot(history_GRU.history['accuracy'], label='GRU')\n",
    "plt.title('LSTM/GRU accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Answer the question:\n",
    "\n",
    "Which part of this task have you been struggling with the most? Describe how you resolved it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Answer the question:\n",
    "\n",
    "Which loss function did you use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__Answer:__</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Build your own LSTMCell (25%)\n",
    "\n",
    "In this part, you will code your own LSTM Cell implementation (including different types of gates that constitute the cell) to build LSTM layers.\n",
    "\n",
    "It is recommended to see the course slides and [`LSTM` source code](https://github.com/keras-team/keras/blob/v2.10.0/keras/layers/rnn/lstm.py). [This link](https://colah.github.io/posts/2015-08-Understanding-LSTMs) also provides a good intuition. \n",
    "\n",
    "![](./img/lstm_cell.png)\n",
    "\n",
    "**Note that this is a simplified figure and not all operations are illustrated.** Please refer to the lectures and follow the equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming some **old** carry state $c_{t-1}$ and hidden state $h_{t-1}$, we try to compute the **new states** given some input $X_t$ using the ideas of LSTM. \n",
    "\n",
    "Remember that for an LSTM cell, we actually have (yellow boxes from left to right) a **forget gate**, an **input gate**, a **cell** and an **output gate**, whose output can be denoted by $f_{t-1}$, $i_{t-1}$, $\\tilde{c}_{t-1}$ and $o_{t-1}$. Each yellow box in the graph denotes a linear projection followed by a particular activation (`sigmoid` or `tanh`). \n",
    "\n",
    "Then for some input $X_t \\in R^{N \\times D}$ and a target output dimension $K$ (also called \"units\"), let's first combine the inputs with the old hidden state $h_{t-1} \\in R^{N \\times K}$ by projecting them into the same dimension \n",
    "\n",
    "$$\n",
    "Z_t = X_t + h_{t-1} W_h\n",
    "$$\n",
    "\n",
    "where $W_h \\in R^{K \\times D}$. Then all new states can be computed one by one as \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "f_t = \\sigma (Z_t W_f) \\\\\n",
    "i_t = \\sigma (Z_t W_i) \\\\\n",
    "\\tilde{c}_t = \\tanh (Z_t W_c) \\\\\n",
    "o_t = \\sigma (Z_t W_o)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where the weights (also called kernels)\n",
    "\n",
    "$$\n",
    "W_f, W_i, W_c, W_o \\in R^{D \\times K}\n",
    "$$\n",
    "\n",
    "Then the **new** carry state is given by the **old carry state** passing through the forget gate and combining with the input\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "and the hidden state is given by the **new carry state** passing through the output gate\n",
    "\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh (c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when computing each state, we are actually computing the four linear projections on $Z_t$, which is **four** projections on $X_t$ and **eight** (because of the $W_h$) projections on $h_{t-1}$. This doesn't look smart enough, so let's rewrite the equations. \n",
    "\n",
    "First define a kernel $W \\in R^{D \\times 4 K}$, which can be thought of as\n",
    "\n",
    "$$\n",
    "W \\gets [W_f, W_i, W_c, W_o]\n",
    "$$\n",
    "\n",
    "Similarly define a recurrent kernel $W_r \\in R^{K \\times 4 K}$, which is equivalent to \n",
    "\n",
    "$$\n",
    "W_r \\gets W_h W = [W_h W_f, W_h W_i, W_h W_c, W_h W_o]\n",
    "$$\n",
    "\n",
    "\n",
    "We can, of course, also add a bias $b \\in R^{4 K}$, and now all the projections simplifies to \n",
    "\n",
    "$$\n",
    "Z_t = X_t W + h_{t-1} W_r + \\mathbb{1} b^T \\in R^{N \\times 4 K}\n",
    "$$\n",
    "\n",
    "We do exactly **one** projection on $X_t$ and **one** on $h_{t-1}$. Then we partition $Z_t$ into \n",
    "\n",
    "$$\n",
    "[z_t^f, z_t^i, z_t^c, z_t^o] \\gets Z_t, \\quad\n",
    "z_t^* \\in R^{N \\times K}\n",
    "$$\n",
    "\n",
    "Easily, \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "f_t = \\sigma (z_t^f) \\\\\n",
    "i_t = \\sigma (z_t^i) \\\\\n",
    "\\tilde{c}_t = \\tanh (z_t^c) \\\\\n",
    "o_t = \\sigma (z_t^o)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the rest stays the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> \n",
    "\n",
    "1. Complete the model `LSTMCell` in **utils/LSTM.py**\n",
    "2. Verify the function with Tensorflow by running the provided code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__NOTE:__</span> \n",
    "\n",
    "You should use the \"simplified\" (second) set of equations as TensorFlow does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veirification Code\n",
    "# Please don't change anything\n",
    "\n",
    "from utils.LSTM import LSTMCell\n",
    "\n",
    "batch_size = 4\n",
    "input_dim = 8\n",
    "units = 64\n",
    "\n",
    "inputs = tf.random.normal((batch_size, input_dim))\n",
    "states = [tf.random.normal((batch_size, units)), tf.zeros((batch_size, units))]\n",
    "\n",
    "# By default, the weights will be initialized randomly (as specified in the code). \n",
    "# Here we try to enforce deterministic (non-random) initializers \n",
    "# so that the two results can be correctly compared. \n",
    "# NOTE: You should NOT do this and leave them to default during actual training\n",
    "\n",
    "lstm_cell = LSTMCell(\n",
    "    units, \n",
    "    kernel_initializer=tf.keras.initializers.Ones, \n",
    "    recurrent_initializer=tf.keras.initializers.Ones, \n",
    "    bias_initializer=tf.keras.initializers.Zeros\n",
    ")\n",
    "h, (_, c) = lstm_cell(inputs, states)\n",
    "\n",
    "lstm_cell_tf = tf.keras.layers.LSTMCell(\n",
    "    units, \n",
    "    kernel_initializer='ones', \n",
    "    recurrent_initializer='ones', \n",
    "    bias_initializer='zeros'\n",
    ")\n",
    "h_tf, (_, c_tf) = lstm_cell_tf(inputs, states)\n",
    "\n",
    "print('Simple verification:')\n",
    "print('Is h correct?', np.allclose(h.numpy(), h_tf.numpy()))\n",
    "print('Is c correct?', np.allclose(c.numpy(), c_tf.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data, out_data = create_xor_dataset(1024)# Create a dataset with a batch size of 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint on building a model with your LSTMCell\n",
    "\n",
    "When creating an RNN with your custom cell, you can just plug in your `LSTMCell` as a building block into a basic `RNN` layer, specify other parameters (e.g. input_shape) as needed, and add it into your model as you did with other layers before. \n",
    "\n",
    "```\n",
    "self.rnn = tf.keras.layers.RNN([YOUR_CELL], **OTHER_ARGS)\n",
    "```\n",
    "\n",
    "For details, please refer to https://www.tensorflow.org/guide/keras/rnn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Complete the model `LSTMModel` in **utils/LSTM.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__NOTE:__</span> \n",
    "\n",
    "We are trying to use a custom layer (our `LSTMCell`) with multiple `call` arguments (\"inputs\" and \"states\"). The pre-building of these types of models is currently NOT allowed in tensorflow. \n",
    "\n",
    "So be sure NOT to call `model.summary()`. Compile and fit it directly and the building will be automatically done in runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Train your `LSTMModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.LSTM import LSTMModel\n",
    "\n",
    "###################################################\n",
    "# TODO: Instantiate your own model and train it.  #\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
